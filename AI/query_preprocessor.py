"""
ë²•ë¥  ì¿¼ë¦¬ ì „ì²˜ë¦¬ í´ë˜ìŠ¤
"""
import functools
from langchain_openai import ChatOpenAI
from config import TERM_MAPPING, LEGAL_INDICATORS, OPENAI_MODEL


class LegalQueryPreprocessor:
    """ì¼ìƒì–´ë¥¼ ë²•ë¥  ìš©ì–´ë¡œ ë³€í™˜í•˜ëŠ” ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=OPENAI_MODEL,
            temperature=0.1,
            max_tokens=200,
        )
        
        self._query_cache = {}
        self.term_mapping = TERM_MAPPING
    
    def _apply_rule_based_conversion(self, query: str) -> str:
        """ë£° ê¸°ë°˜ ìš©ì–´ ë³€í™˜"""
        converted_query = query
        for common_term, legal_term in self.term_mapping.items():
            if common_term in converted_query:
                converted_query = converted_query.replace(common_term, legal_term)
        return converted_query
    
    def _is_already_legal_query(self, query: str) -> bool:
        """ì´ë¯¸ ë²•ë¥  ìš©ì–´ì¸ì§€ í™•ì¸"""
        return any(term in query for term in LEGAL_INDICATORS)
    
    @functools.lru_cache(maxsize=100)
    def _gpt_convert_to_legal_terms(self, user_query: str) -> str:
        """GPTë¥¼ ì´ìš©í•œ ë²•ë¥  ìš©ì–´ ë³€í™˜"""
        try:
            prompt = f"""ë‹¤ìŒ ì¼ìƒì–´ ì§ˆë¬¸ì„ ë²•ë¥  ê²€ìƒ‰ì— ì í•©í•œ ì „ë¬¸ ìš©ì–´ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
            ì›ë˜ ì§ˆë¬¸: {user_query}
            ë³€í™˜ ê·œì¹™:
            1. ì¼ìƒì–´ë¥¼ ì •í™•í•œ ë²•ë¥  ìš©ì–´ë¡œ ë°”ê¾¸ê¸°
            2. í•µì‹¬ ë²•ì  ìŸì ì„ ë¶€ê°ì‹œí‚¤ê¸°
            3. ê²€ìƒ‰ì— ë„ì›€ì´ ë˜ëŠ” ê´€ë ¨ ë²•ë¥  í‚¤ì›Œë“œ ì¶”ê°€
            4. ì›ë˜ ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë©´ì„œ ë” ì •í™•í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ í‘œí˜„
            ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬:"""

            messages = [{"role": "user", "content": prompt}]
            response = self.llm.invoke(messages)
            
            converted = response.content.strip()
            if "ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬:" in converted:
                converted = converted.split("ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬:")[-1].strip()
            
            return converted
            
        except Exception as e:
            print(f"âš ï¸ GPT ë³€í™˜ ì‹¤íŒ¨, ë£°ë² ì´ìŠ¤ ë³€í™˜ ì‚¬ìš©: {e}")
            return self._apply_rule_based_conversion(user_query)
    
    def convert_query(self, user_query: str) -> tuple[str, str]:
        """ì¿¼ë¦¬ ë³€í™˜ ë©”ì¸ í•¨ìˆ˜"""
        try:
            if self._is_already_legal_query(user_query):
                return user_query, "no_conversion"
            
            if user_query in self._query_cache:
                return self._query_cache[user_query], "cached"
            
            rule_converted = self._apply_rule_based_conversion(user_query)
            
            if len(rule_converted) != len(user_query) or rule_converted != user_query:
                self._query_cache[user_query] = rule_converted
                return rule_converted, "rule_based"
            
            print("ğŸ”„ ì •êµí•œ ë²•ë¥  ìš©ì–´ ë³€í™˜ ì¤‘...")
            gpt_converted = self._gpt_convert_to_legal_terms(user_query)
            
            self._query_cache[user_query] = gpt_converted
            return gpt_converted, "gpt_converted"
            
        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ë³€í™˜ ì˜¤ë¥˜: {e}")
            return user_query, "error"
